{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station List Data : data was pulled to get longitude and latitude to the weather data which only had corresponding stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/stations.txt','r')\n",
    "var_list = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for x in range(len(var_list)):\n",
    "    new_list.append(var_list[x].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df = pd.DataFrame(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df = key_df.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df.rename(columns = {0:'station_id',1:'latitude',2:'longitude',3:'elevation',4:'state_real'},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Weather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data sets only had 2 columns. The rest was configured with rows of the same day but with different features. This had to be changed and altered to be configured for columned data. Ultimately all years of weather data turned out to be 30 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather(file_name):\n",
    "    df = pd.read_csv(f'./data/raw_weather/{file_name}', header = None)\n",
    "    #dropping unused columns\n",
    "    df = df.drop(columns = [4,5,6,7])\n",
    "    # seperating data from ditinctions in rows min and max temperature\n",
    "    df = df.loc[df[2].isin(['TMIN','TMAX'])]\n",
    "    df_tmax = df.loc[df[2]=='TMAX']\n",
    "    df_tmin = df.loc[df[2]=='TMIN']\n",
    "    #adjusting row data to now be columns\n",
    "    df_tmax.rename(columns = {0:'station_id',1:'date',2:2,3:'tmax'}, inplace = True)\n",
    "    df_tmin.rename(columns = {0:'station_id',1:'date',2:2,3:'tmin'}, inplace = True)\n",
    "    df_tmax = df_tmax.drop(columns = 2)\n",
    "    df_tmin = df_tmin.drop(columns = 2)\n",
    "    #changing data to datetime\n",
    "    df_tmin['date'] = pd.to_datetime(df_tmin['date'], format='%Y%m%d')\n",
    "    df_tmax['date'] = pd.to_datetime(df_tmax['date'], format='%Y%m%d')\n",
    "    df_total = pd.merge(df_tmin,df_tmax, on = ['station_id','date'])\n",
    "    final_df = pd.merge(left = df_total,right = key_df,how = 'inner',on = 'station_id')\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_to_csv(file_name):\n",
    "    df = clean_weather(file_name)\n",
    "\n",
    "    return df.to_csv(f'./data/clean_weather/true_cleaned{file_name[:4]}.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 csv done\n",
      "2001 csv done\n",
      "2002 csv done\n",
      "2003 csv done\n",
      "2004 csv done\n",
      "2005 csv done\n",
      "2006 csv done\n",
      "2007 csv done\n",
      "2008 csv done\n",
      "2009 csv done\n",
      "2010 csv done\n",
      "2011 csv done\n",
      "2012 csv done\n",
      "2013 csv done\n",
      "2014 csv done\n",
      "2015 csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 csv done\n",
      "2017 csv done\n",
      "2018 csv done\n"
     ]
    }
   ],
   "source": [
    "#running all years\n",
    "for x in range(2000,2019):\n",
    "    clean_weather_to_csv(f'{x}.csv.gz')\n",
    "    print(f'{x} csv done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_00_18 = [pd.read_csv(f'./data/clean_weather/true_cleaned{x}.csv') for x in range(2000,2019)]\n",
    "\n",
    "df = pd.concat(df_list_00_18)\n",
    "\n",
    "df.to_csv('./data/true_total_weather.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total this is 5 gb of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combing Fire and Weather data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds the temperatures and locations for the fires with station ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.read_csv('./data/fire.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = pd.read_csv('./data/true_total_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['state_real'] = fdf['Fire_ID,C,254'].map(lambda x : x[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['IG_Date,D'] = pd.to_datetime(fdf['IG_Date,D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.rename(columns = {'IG_Date,D':'date'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['date'] = pd.to_datetime(wdf['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['tmin'] = wdf['tmin'].map(lambda x : x * .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['tmax'] = wdf['tmax'].map(lambda x : x * .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['temp_avg'] =( wdf['tmax'] + wdf['tmin'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['daily_state_temp'] = wdf.groupby(['date','state_real'])[['temp_avg']].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.to_csv('./data/final_wdf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = wdf.drop_duplicates(['daily_state_temp','date','state_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.reset_index(drop = 'index',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wdf['daily_state_temp']=wdf['daily_state_temp'].map(lambda x : (x * 9/5) + 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = wdf[['daily_state_temp','date','state_real']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fdf = pd.merge(left = fdf, right = wdf, on =['state_real','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fdf.to_csv('./data/adjusted_fire.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
